\renewcommand{\arraystretch}{1.25}
\subsection*{Applications of the SVD}
\textbf{Principal Component Analysis}

\textit{Principal Component Analysis} (PCA) is a procedure that uses the SVD to analyze data by finding the directions of maximum "spread" or variation. Some of its uses include visualizing high dimensional data and revealing patterns that are useful to perform classification.\\
\newline
Below, we show the steps of performing PCA, as well as visualizations on a two dimensional dataset:

\includegraphics[width=\textwidth]{figures/pca-steps}

In summary,
\begin{enumerate}
    \item We first \textbf{demean} the data. If each datapoint is represented as a \textit{row}, then we are going to want to find the mean of each \textit{column} and subtract it from every element in that column to make it \textbf{zero-mean}. If each datapoint is a \textit{column}, we want every \textit{row} to be zero-mean.
    \item If each datapoint is a \textit{row} (as in the previous example), the covariance matrix is $\frac{1}{n} \widetilde{X}^T\widetilde{X}$, and the \textit{principal components} are the \textbf{columns of $V$} in the SVD (eigenvectors of the covariance matrix). 
    If each datapoint is a \textit{column}, the covariance matrix is $\frac{1}{n} \widetilde{X}\widetilde{X}^T$, and the \textit{principal components} correspond to the \textbf{columns of $U$}. \\
    \newline
    You can think of the covariance matrix as looking at the relation between the quantities we're measuring, and its eigenvectors as the directions in our data with the most variation.
    \item When you have the principal components, you can \textbf{project datapoints onto the principal components} to see how much of each principal component contributes to that datapoint.
\end{enumerate}

\newpage
\textbf{Minimum Norm Control} \\
Say we have a controllable system of rank $n$:
\begin{align*}
    \vec{x}(k + 1) = A\vec{x}(k) + \vec{b}u(k)
\end{align*}
 and we want to reach a desired state $\vec{x}_f$ with $k > n$ control inputs. We know we can reach any state in $n$ timesteps, and there are infinitely many ways to reach $\vec{x}_f$ in $k > n$ timesteps. Using the SVD however, we can find the series of control inputs that has the minimum norm. \\
 \newline
 Recall from the section on control that we can use the following linear system to solve for the control inputs:
 \begin{align*}
    \vec{x}(k) = \mathcal{C}_k \vec{u}_k
\end{align*}
\begin{align*}
    \vec{x}(k) = \begin{bmatrix}
        \vec{b} & A\vec{b} & \cdots & A^{k - 1} \vec{b}
    \end{bmatrix} \begin{bmatrix}
        u(k - 1) \\ \vdots \\ u(0)
    \end{bmatrix}
 \end{align*}

To solve for the minimum norm solution, we use the \textbf{Moore-Penrose Pseudoinverse}, a generalization of matrix inverses for rectangular matrices using the full SVD of a matrix. We denote the pseudoinverse with a dagger, and the solution will be $\vec{u}_k = \mathcal{C}_k^{\dagger} \vec{x}(k).$ 
This solution is special because it has \textbf{minimum norm}; that is, if we have another solution $\vec{z},$ such that $\mathcal{C}_k \vec{z} = \vec{x}(k),$ then $\norm{\vec{u}_k} \leq \norm{z}.$

To compute the pseudoinverse, we invert each element of the SVD one at a time:
\begin{align*}
    \mathcal{C}_k \vec{u}_k &= \vec{x}(k) \\
    U \Sigma V^T \vec{u}_k &= \vec{x}(k)\\
    (U^T U) \Sigma V^T \vec{u}_k &= U^T \vec{x}(k) \\
    (\Sigma^{\dagger} \Sigma) V^T \vec{u}_k &= \Sigma^{\dagger} U^T \vec{x}(k) \\
    (V V^T) \vec{u}_k &= V \Sigma^{\dagger} U^T \vec{x}(k) \\
    \vec{u}_k &= V \Sigma^{\dagger} U^T \vec{x}(k) \\
    \implies \vec{u}_k &= \mathcal{C}_k^{\dagger} \vec{x}(k)
\end{align*}

We can calculate $\Sigma^{\dagger}$ accordingly to produce an identity matrix such that the term $\Sigma^{\dagger}\Sigma$ disappears:

$$\Sigma^{\dagger} = \begin{bmatrix} \frac{1}{\sigma_{1}} & 0 &  \cdots & 0 \\ 0 & \frac{1}{\sigma_{2}} & \cdots & 0 \\ \vdots & \vdots & \ddots & \frac{1}{\sigma_{m}} \\ 
    \vdots & \vdots & \ddots & 0 \\ 0 & 0 & 0 & 0 \end{bmatrix}$$

\textit{Note}: This is another, sometimes computationally easier, form of the pseudoinverse:
$$\vec{u}_k = \mathcal{C}_k^T(\mathcal{C}_k\mathcal{C}_k^T)^{-1}\vec{x}(k)$$
See Sp20: Note 11B for more information.

\newpage

\subsection*{Signals}
\textbf{Sampling} \\

\textbf{Convolution} \\
With a discrete-time signal in our grasp, we will want to 

\textbf{Interpolation} \\
We saw that \textbf{sampling} lets us take a signal defined in continuous-time and convert it to a discrete-time representation.
The inverse problem to this is \textbf{interpolation}, which is all about going from a discrete-time representation of a signal $x[i]$ to the continuous-time representation $x(t)$.

\includegraphics[width=\textwidth]{figures/interp_ex}

We may formalize the problem as follows: Given a discrete-time signal $x[i]$ with sampling period $\Delta$ (and therefore sampling frequency $\frac{1}{\Delta}$), we aim to generate a continuous-time signal $x(t)$ such that 

\begin{equation*}
\forall i \in \mathbb{Z}, x(i \Delta) = x[i]
\end{equation*}

In words, this means that our interpolated signal $x(t)$ must go through each sample from the discrete-time signal $x[i]$.
This is the main point which causes interpolation to differ from \textbf{regression}.
With regression, we also seek to extrapolate information from sampled data, however we don't seek to exactly fit the sampled data.
Instead, we aim to find the best approximation.
On the other hand, for a continuous-time signal to be a valid interpolation of a discrete-time signal, the continuous time signal \textbf{must} pass through each point specified by the discrete-time signal.
\newline
There are many ways to go about interpolation, but for 16B many of these ways all follow the same form.

\textbf{Basis Functions} \\
Recall that for a given vector space, we can form any vector in that vector space with a linear combination of a set of basis vectors.
Similarly, we are going to view interpolation through the lens of representing a continuous time signal as a linear combination of shifted basis functions, with weights determined by values of the of the discrete time signal.
\newline
For a basis function $\phi(t)$ to be valid, it must satisfy

\begin{equation*}
    \phi(0) = 1
\end{equation*}
and
\begin{equation*}
    \phi(k \Delta) = 0, k \in \mathbb{Z} \setminus 0
\end{equation*}

With these conditions satisfied, $\phi(t)$ produces the following interpolation: 

\begin{equation*}
    x(t) = \sum_{k=-\infty}^{k=\infty} x[k] \phi(t - k \Delta)
\end{equation*}

Essentially, for every sample in our discrete-time signal, we shift our basis function to that sample, scale by the value of the sample, and and add together all these shifted and scaled basis functions to produce our interpolation. The previously established conditions on $\phi(t)$ ensure that this shifting, scaling, and adding results in a valid interpolation, which we prove by confirming $x(i \Delta) = x[i]$.
\begin{equation*}
    x(i \Delta) = \sum_{k=-\infty}^{k=\infty} x[k] \phi(i \Delta - k \Delta
\end{equation*}
Using the defined properties of our basis function $\phi(t)$, we can simplify the summation.
\begin{align*}
\phi(i \Delta - k \Delta)
    &=
    \begin{cases}
        1 & i = k \\
        0 & \text{otherwise} \\
    \end{cases} \\
\sum_{k=-\infty}^{k=\infty} x[k] \phi(i \Delta - k \Delta) &= x[i] \\
x(i \Delta) &= x[i]
\end{align*}

Therefore, basis functions give us a valid interpolation. We will see different ways we can define our basis funtion to get many different interpolations, each with their own strenghts and drawbacks.

\textbf{Zero-Order Hold Interpolation} \\
The zero-order hold is the the interpolation we get when we hold the value of each sample in our discrete-time signal constant over the sampling interval.

\includegraphics[width=\textwidth]{figures/zoh_ex}

Formally, we may define the interpolation as
\begin{equation*}
    \forall i \in \mathbb{Z}, x(t) = x[i],\;t \in [i \Delta, (i + 1) \Delta
\end{equation*}

We can also define this interpolation using the basis function formulation.
\begin{equation*}
    \phi(t) = 
    \begin{cases}
    1 & t \in [0, \Delta) \\
    0 & \text{otherwise} \\
    \end{cases}
\end{equation*}

\textbf{Linear Interpolation} \\
\includegraphics[width=\textwidth]{figures/lin_basis}


\textbf{Polynomial Interpolation} \\

\textbf{Sinc Interpolation} \\
\includegraphics[width=\textwidth]{figures/sinc_basis}





\textbf{Nyquist Sampling Theorem and Aliasing} \\



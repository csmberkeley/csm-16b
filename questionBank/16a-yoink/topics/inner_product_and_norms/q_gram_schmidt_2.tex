\qns{Gram-Schmidt}

\meta{
Ask students why gram-schmidt might be useful. Make sure students know the connection of Gram-Schmidt to linear dependence/independence.
} 
Consider an arbitrary set $S$ of linearly-independent vectors. Recall that vectors are "orthogonal" when the angle between them is 90$^\circ$, and vectors of "unit length" have magnitude = 1.

The term "orthonormal" collapses these two terms into one: a set of vectors is orthonormal \textbf{if and only if}:
    \begin{itemize}
        \item Any pair of vectors are orthogonal (for any vectors $u, v \in S$ where $u \neq v$, the angle between them is 90$^\circ$).
        \item Each vector is of unit length (for vector $v \in $ $S$, $|v| = 1$).
    \end{itemize}

Gram-Schmidt Orthonormalization is a process that allows us to take such a set $S$ of arbitrary, linearly-independent vectors, and create a new set $S'$ of vectors that span the exact same space, yet are now orthonormal.

\begin{enumerate}

% \item Let us go back to EECS 16A. Suppose we have some equation $A\vec{x} = \vec{b}$ that has no ansution. Why does it have no solution? Because $A$ has more rows than columns; it is a "tall" matrix where we have more equations than unknowns. How can we solve for $x$? In other words, what is the formula for least-squares?

% \ans {  
% Multiply both sides by $A^{T}$ to get:
% $$A^TA\vec{x} = A^T\vec{b}$$
% $A^TA$ is a square matrix that we can show to be invertible (?). So, we multiply both sides by the inverse to get
% $$\vec{x} = (A^TA)^{-1}A^T\vec{b}$$
% This is the formula for least-squares from EECS 16A.
% }

% \item From the result above, we see that we have to invert $A^TA$ every time. This takes a lot of computational power and is rather slow. If $A$ was an orthonormal matrix, how could this help us?

% \ans {
% Let $A$ be orthonormal. If this were the case, then $A^TA = I$. Recall that $I^{-1} = I$. This means we don't have to do any work for inverting this matrix, and the least-squares formula reduces to $\vec{x} = A^T\vec{b}$.
% }

% \item We would like $A$ to be orthonormal. We cannot, however, arbitrarily replace $A$ with an orthonormal matrix $A'$. What must be true about $A'$ for the least-squares equation to hold?

% \ans {
% Both $A$ and $A'$ must span the same space. Geometrically, least-squares projects $\vec{b}$ onto the column space (aka span) of $A$, so if we want the projection to hold when using $A'$, then they must have the same span.
% }

\item Justify that span($\vec{v_1}, \vec{v_2}$) is the same as span($\vec{v_1}, \vec{v_2} - \alpha\vec{v_1}$).

\ans {
The definition of span is all vectors that can be "reached" using a linear combination of a set of vectors. So,
$$span(\vec{v_1}, \vec{v_2}) = \vec{v}_{i}=\beta_{1} \vec{v}_{1}+\beta_{2} \vec{v}_{2} \text { for arbitrary } \vec{v}_{i} \text { and all real } \beta_{1}, \beta_{2}$$
Following a similar train of thought,
$$\begin{aligned} \operatorname{span}\left(\vec{v}_{1}, \vec{v}_{2}-\alpha \vec{v}_{1}\right) &=\beta_{3} \vec{v}_{1}+\beta_{4}\left(\vec{v}_{2}-\alpha \vec{v}_{1}\right) \\ &=\beta_{3} \vec{v}_{1}+\beta_{4} \vec{v}_{2}-\beta_{4} \alpha \vec{v}_{1} \\ &=\left(\beta_{3}-\beta_{4} \alpha\right) \vec{v}_{1}+\beta_{4} \vec{v}_{2} \end{aligned}$$
All $\alpha$ and $\beta$ are arbitrary constants that we can set. If we set $\beta_4 = \beta_2$ and $\beta_3 = \beta_1 + \alpha\beta_2$, then these spans are the same!
}

\item Let us iteratively generate the orthonormal set $S'$ from $S$. Start by arbitrarily picking a vector $\vec{v_1}$ from $S$: how can we create a new vector $\vec{w_1}$ for $S'$ so that $\vec{w_1}$ spans the same space as the original vector, yet ensures that orthonormality is preserved for our new set?

\ans {
 Since we are only taking into consideration a single vector, we do not need to worry about orthogonality. So,
$$\vec{w_1} = \frac{\vec{v_1}}{|\vec{v_1}|}$$
}

\item Now, consider an arbitrary vector $\vec{v_2}$ in $S$. How can we create $\vec{w_2}$ for our orthonormal set?

\ans {
Now, we do need to consider orthogonality. In order for two vectors $\vec{u}, \vec{v}$ to be orthogonal, their dot product has to be 0. In other words, this means that there is nothing--no trace, no component--of $\vec{u} \text { in } \vec{v}$, and vice versa. We can do this by defining the following:
$$\vec{p_2} = \vec{v_2} - \langle\vec{v_2}, \vec{w_1}\rangle\vec{w_1}$$
By subtracting the projection of $\vec{v_2}$ onto $\vec{w_1}$, we are removing the $\vec{w_1}$ component of $\vec{v_2}$ from $\vec{v_2}$. So, $\vec{p_2}$ is now orthogonal to $\vec{w_1}$, and mutual orthogonality is preserved for all vectors in our new set $S'$. $\vec{p_2}$ also needs to be normalized, so:
$$\vec{w_2} = \frac{\vec{p_2}}{|\vec{p_2}|}$$
}

\item Write out the process for creating an arbitrary vector $\vec{w_i}$ for our orthonormal set $S'$.

\ans {
Define the following vector:
$$\vec{p_i} = \vec{v_i} - \sum_{j=1}^{i-1}(\vec{v_i}^T\vec{w_j})\vec{w_j}$$
By subtracting the component of every other vector already in $S'$, this ensures that our new vector $\vec{w_i}$ is orthogonal. Then, we normalize this:
 $$\vec{w_i} = \frac{\vec{p_i}}{|\vec{p_i}|}$$
Rinse and repeat for all vectors in $S$ to generate a new, orthonormal set $S'$ that we can use in place of $S$.
}
\end{enumerate}






% A set of vectors is orthonormal \textbf{if and only if}:
% \begin{itemize}
%     \item Any pair of vectors are orthogonal (for any vectors $\vec{u}, \vec{v} \in S$ where $\vec{u} \neq \vec{v}$, the angle between them is 90$^\circ$).
%     \item Each vector is of unit length (for vector $\vec{v} \in $ $S$, $\norm{\vec{v}} = 1$).
% \end{itemize}

% Gram-Schmidt orthonormalization is a process that allows us to take such a set $S$ of arbitrary, linearly-independent vectors, and create a new set $S'$ of vectors that span the exact same space, yet are now orthonormal.

% \begin{enumerate}

% \item For linearly independent vectors $\vec{v}_1, \vec{v}_2$, \textbf{justify that Span($\vec{v}_1, \vec{v}_2$) is the same as Span($\vec{v}_1, \vec{v}_2 - \alpha\vec{v}_1$).}

% %\ws{\vspace{75px}}
% \ans {
% The definition of span is all vectors that can be "reached" using a linear combination of a set of vectors. So,
% $$span(\vec{v_1}, \vec{v_2}) = \vec{v}_{i}=\beta_{1} \vec{v}_{1}+\beta_{2} \vec{v}_{2} \text { for arbitrary } \vec{v}_{i} \text { and all real } \beta_{1}, \beta_{2}$$
% Following a similar train of thought,
% $$\begin{aligned} \operatorname{span}\left(\vec{v}_{1}, \vec{v}_{2}-\alpha \vec{v}_{1}\right) &=\beta_{3} \vec{v}_{1}+\beta_{4}\left(\vec{v}_{2}-\alpha \vec{v}_{1}\right) \\ &=\beta_{3} \vec{v}_{1}+\beta_{4} \vec{v}_{2}-\beta_{4} \alpha \vec{v}_{1} \\ &=\left(\beta_{3}-\beta_{4} \alpha\right) \vec{v}_{1}+\beta_{4} \vec{v}_{2} \end{aligned}$$
% All $\alpha$ and $\beta$ are arbitrary constants that we can set. If we set $\beta_4 = \beta_2$ and $\beta_3 = \beta_1 + \alpha\beta_2$, then these spans are the same!
% }

% \item Let us iteratively generate the orthonormal set $S'$ from $S$. Start by arbitrarily picking a vector $\vec{v}_1$ from $S$: \textbf{how can we create a new vector $\vec{u}_1$ for $S'$ so that it spans the same space as the original vector, yet ensures that orthonormality is preserved for our new set?}

% %\ws{\vspace{75px}}

% \ans {
%  Since we are only taking into consideration a single vector, we do not need to worry about orthogonality. So,
% $$\vec{u}_1 = \frac{\vec{v}_1}{\norm{\vec{v}_2}}$$
% }

% \item Now, consider the next vector $\vec{v}_2$ in $S$. \textbf{How can we convert $\vec{v}_2$ to some $\vec{u}_2$ that is orthonormal with respect to the $\vec{u}_1$ that we've already added to $S'$?}

% %\ws{\vspace{75px}}

% \ans {
% Now, we do need to consider orthogonality. In order for two vectors $\vec{u}, \vec{v}$ to be orthogonal, their dot product has to be 0. In other words, this means that there is nothing--no trace, no component--of $\vec{u} \text { in } \vec{v}$, and vice versa. We can do this by defining the following where:
% $$\vec{p_2} = \vec{v_2} - (\vec{v_2}^T\vec{u_1})\vec{u_1}$$
% By subtracting the projection of $\vec{v_2}$ onto $\vec{u_1}$, we are removing the $\vec{u_1}$ component of $\vec{v_2}$ from $\vec{v_2}$. So, $\vec{p_2}$ is now orthogonal to $\vec{u_1}$, and mutual orthogonality is preserved for all vectors in our new set $S'$. $\vec{p_2}$ also needs to be normalized, so:
% $$\vec{w_2} = \frac{\vec{p_2}}{|\vec{p_2}|}$$
% }

% \item \textbf{Write out the process for creating an arbitrary vector $\vec{u}_i$ for our orthonormal set $S'$.}

% %\ws{\vspace{75px}}

% \ans {
% Define the following vector:
% $$\vec{p_i} = \vec{v_i} - \sum_{j=1}^{i-1}(\vec{v_i}^T\vec{w_j})\vec{w_j}$$
% By subtracting the component of every other vector already in $S'$, this ensures that our new vector $\vec{w_i}$ is orthogonal. Then, we normalize this:
%  $$\vec{w_i} = \frac{\vec{p_i}}{|\vec{p_i}|}$$
% Rinse and repeat for all vectors in $S$ to generate a new, orthonormal set $S'$ that we can use in place of $S$.
% }

% \item
% Let $A = \begin{bmatrix}
% -1 & -1 & 1 \\
% 1 & 3 & 3 \\
% -1 & -1 & 5 \\
% 1 & 3 & 7 \end{bmatrix}$.

% We will use Gram-Schmidt to find an orthonormal basis $U = \begin{bmatrix} \vec{u}_{1} & \vec{u}_{2} & \vec{u}_{3} \end{bmatrix}$ for the $\text{Col}(A).$

% \begin{enumerate}
%   \item \textbf{Find $\vec{u}_{1}$.}

%   \meta {
%     It really is up to you, how you teach Gram-Schmidt, but I prefer to make all of the vectors orthogonal first, and then normalize at the end, once I've found an orthogonal set of vectors $\{ \vec{q}_{1}, \cdots \vec{q}_{n} \}.$
%   }

%   %\ws{\vspace{75px}}
%   \ans {
%     For the first vector, we do not need to worry about orthogonality. Therefore we just have to normalize.
%     $$\vec{u}_{1} = \frac{\vec{a}_{1}}{\norm{\vec{a}_{1}}} = \frac{1}{\sqrt{4}} \begin{bmatrix} -1 \\ 1 \\ -1 \\ 1 \end{bmatrix} =
%     \begin{bmatrix} -\frac{1}{2} \\ \frac{1}{2} \\ -\frac{1}{2} \\ \frac{1}{2} \end{bmatrix}$$
%   }

%   \item \textbf{Find $\vec{u}_{2}$.}

%   %\ws{\vspace{75px}}
%   \ans {
%     We first compute $\vec{q}_{2}$ as a vector orthogonal to $\vec{u}_{1}$ by subtracting the projection of $\vec{a}_{2}$ onto $\vec{u}_{1}$
%     $$\vec{q}_{2} = \vec{a}_{2} - \langle \vec{a}_{2}, \vec{u}_{1}\rangle \vec{u}_{1} = \vec{a}_{2} - \frac{1}{2} \langle\vec{a}_{2}, \vec{a}_{1}\rangle \frac{1}{2} \vec{a}_{1}
%     = \begin{bmatrix} -1 \\ 3 \\ -1 \\ 3 \end{bmatrix} -
%     \frac{8}{4} \begin{bmatrix} -1 \\ 1 \\ -1 \\ 1 \end{bmatrix} = \begin{bmatrix} 1 \\ 1 \\ 1 \\ 1 \end{bmatrix}$$
%     If we were to normalize this vector, we would get:
%     $$\vec{u}_{2} = \frac{1}{\sqrt{4}} \begin{bmatrix} 1 \\ 1 \\ 1 \\ 1 \end{bmatrix} =
%     \begin{bmatrix} \frac{1}{2} \\ \frac{1}{2} \\ \frac{1}{2} \\ \frac{1}{2} \end{bmatrix}$$
%   }

%   \item \textbf{Find $\vec{u}_{3}$.}

%   %\ws{\vspace{100px}}
%   \ans {
%     We again compute $\vec{q}_{3}$ as a vector orthogonal to rest by subtracting its projections onto $\vec{a}_{1}$ and $\vec{a}_{2}.$
%     \begin{align*}
%     \vec{q}_{3} &= \vec{a}_{3} - \langle\vec{a}_{3}, \vec{u}_{1}\rangle \vec{u}_{1} - \langle\vec{a}_{3}, \vec{u}_{2}\rangle \vec{u}_{2} =
%     \vec{a}_{3} - \frac{1}{4} \langle\vec{a}_{3}, \vec{a}_{1}\rangle \vec{a}_{1} - \frac{1}{4} \langle\vec{a}_{3}, \vec{q}_{2}\rangle \vec{q}_{2} \\
%     &= \begin{bmatrix} 1 \\ 3 \\ 5 \\ 7 \end{bmatrix} - \frac{4}{4} \begin{bmatrix} -1 \\ 1 \\ -1 \\ 1 \end{bmatrix} -
%     \frac{16}{4} \begin{bmatrix} 1 \\ 1 \\ 1 \\ 1 \end{bmatrix} = \begin{bmatrix} 1 \\ 3 \\ 5 \\ 7 \end{bmatrix} - \begin{bmatrix} -1 \\ 1 \\ -1 \\ 1 \end{bmatrix} - \begin{bmatrix} 4 \\ 4 \\ 4 \\ 4 \end{bmatrix} = \begin{bmatrix} -2 \\ -2 \\ 2 \\ 2 \end{bmatrix}
%     \end{align*}
%     Normalizing $\vec{q}_{3},$ we get:
%     $$\vec{u}_{3} = \frac{1}{\sqrt{16}} \begin{bmatrix} -2 \\ -2 \\ 2 \\ 2 \end{bmatrix} = \begin{bmatrix} -\frac{1}{2} \\ -\frac{1}{2} \\ \frac{1}{2} \\ \frac{1}{2} \end{bmatrix}$$
%   }

% \end{enumerate}

%\end{enumerate}
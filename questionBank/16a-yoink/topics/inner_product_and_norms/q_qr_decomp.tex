\qns{QR Decomposition}

While we have this amazing speedup through Gram-Schmidt, it is important to realize that our current signals $\vec{q}_{i}$ were modified to be orthonormal, when the actual signals $\vec{s}_{i}$ were not necessarily orthogonal. Our current solution $\hat{x}$ is in $Q$ basis coordinates as opposed to standard basis coordinates. Therefore, to tackle this, we will use a concept called QR Decomposition. 

\begin{enumerate}

  \item The $Q_{i}$ matrix in the previous question was constructed by performing Gram-Schmidt on the columns of $S_{i}.$
  We also know that the columns of the $Q_{i}$ matrix form a basis for the $\text{Col}(S_{i}).$ How can you represent the columns of $S_{i}$ using the basis $\{\vec{q}_{1}, \cdots, \vec{q}_{i} \}?$

  \ans{
    Since the columns of $Q_{i}$ form a basis for the $\text{Col}(S_{i}),$ any vector in $\text{Col}(S_{i}),$ can be represented as a linear combination of $\{\vec{q}_{1}, \cdots, \vec{q}_{i}\}.$
    $$\vec{s}_{1} = \alpha_{1} \vec{q}_{1} + \dotsc + \alpha_{i} \vec{q}_{i}$$
    We can solve for these $\alpha$ coefficients by taking the inner product of both sides with $\vec{q}_{1}.$
    \begin{align*}
    \langle\vec{q}_{1}, \vec{s}_{1}\rangle &= \langle\vec{q}_{1}, \alpha_{1} \vec{q}_{1} + \dotsc + \alpha_{i} \vec{q}_{i}\rangle \\
    &= \alpha_{1} \langle\vec{q}_{1}, \vec{q}_{1}\rangle + \dotsc + \alpha_{i} \langle\vec{q}_{1}, \vec{q}_{i}\rangle \\
    &= \alpha_{1} \langle vec{q}_{1}, \vec{q}_{1}\rangle = \alpha_{1}
    \end{align*}
    We can do this similarly to solve for any $\vec{s}_{j}$ to say that
    $$\vec{s}_{j} = \langle\vec{q}_{1}, \vec{s}_{j}\rangle \vec{q}_{1} + \dotsc + \langle\vec{q}_{j}, \vec{s}_{j}\rangle \vec{q}_{j}$$
  }

  \item While we have derived an expression for the coefficients $\alpha_{j},$ we can realize that most of the coefficients will be zero due to the way $q_{j}$ was constructed from $S.$ Try writing out the vectors $\vec{q}_{1}, \vec{q}_{2}, \vec{q}_{3}$ using the Gram-Schmidt algorithm, to conclude which coefficients will be zero.

  \ans{
    Let's first look at the vector $\vec{q}_{1}.$ We notice that $\vec{s}_{1}$ only depends on $\vec{q}_{1}.$
    $$\vec{q}_{1} = \frac{\vec{s}_{1}}{\norm{\vec{s}_{1}}}$$
    Therefore, only $\alpha_{1}$ will be nonzero, and all of the other $\alpha_{j}$ will be zero, due to the orthogonality of $\vec{q}_{j}.$ \\
    Now let's look at the vector $\vec{q}_{2}.$
    $$\vec{q}_{2} = \vec{s}_{2} - \langle\vec{s}_{2}, \vec{q}_{1}\rangle \vec{q}_{1}$$
    Solving for $\vec{s}_{2},$ we see that it is only in terms of $\vec{q}_{1}$ and $\vec{q}_{2}$
    $$\vec{s}_{2} = \langle\vec{s}_{2}, \vec{q}_{1}\rangle \vec{q}_{1} + \vec{q}_{2}$$
    Again, we conclude that $\alpha_{1}$ and $\alpha_{2}$ will be nonzero, but all other $\alpha_{j}$ will be zero.
    Now we look at $\vec{q}_{3}.$ We can again solve for $\vec{s}_{3}$ and conclude that it is only in terms of $\vec{s}_{1}, \vec{s}_{2}, \vec{s}_{3}.$
    \begin{align*}
    \vec{q}_{3} &= \vec{s}_{3} - \langle\vec{s}_{3}, \vec{q}_{1}\rangle \vec{q}_{1} - \langle\vec{s}_{3}, \vec{q}_{2}\rangle \vec{q}_{2} \\
    \vec{s}_{3} &= \langle\vec{s}_{3}, \vec{q}_{1}\rangle \vec{q}_{1} + \langle\vec{s}_{3}, \vec{q}_{2}\rangle \vec{q}_{2} + \vec{q}_{3} 
    \end{align*}
    We can continue doing this, and will continue noticing that for $\vec{s}_{j},$ all of the $\alpha_{1}, \cdots, \alpha_{j}$ will be nonzero, and $\alpha_{j+1}, \cdots, \alpha_{i}$ will be zero.
  }

  \item Using the expression derived from the previous part, how can you represent the vector $\vec{s}_{j}$ as a matrix-vector equation $\vec{s}_{j} = Q_{i} \vec{r}_{j}$

  \ans{
    Rewriting out the expression for $\vec{s}_{j},$ we get:
    $$\vec{s}_{j} = \langle\vec{q}_{1}, \vec{s}_{j}\rangle \vec{q}_{1} + \dotsc + \langle\vec{q}_{j}, \vec{s}_{j}\rangle \vec{q}_{j}$$
    Remember that for any matrix-vector equation can be written as a linear combination of the columns and vice versa.
    $$\vec{s}_{j} = Q_{j} \vec{r}_{j} \begin{bmatrix}
    | & | & | \\
    \vec{q}_{1} & \cdots & \vec{q}_{j} \\
    | & | & |
    \end{bmatrix} \begin{bmatrix} \langle\vec{q_1}, \vec{q_1}\rangle \\ \vdots \\ \langle\vec{q_j}, \vec{s_j}\rangle \end{bmatrix}$$
    However, this matrix $Q_{j}$ only has $j$ columns, and we need to extend it to have $i$ columns.
    Noting that the coefficients $\alpha_{j + 1}, \cdots, \alpha_{i}$ will be $0$ for $\vec{q}_{j + 1}, \cdots, \vec{q}_{i},$ we can represent $\vec{r}_{j}$ as the following:
    $$\vec{r}_{j} = \begin{bmatrix} \langle\vec{q}_{1}, \vec{s}_{j}\rangle \\ \vdots \\ \langle\vec{q}_{j}, \vec{s}_{j}\rangle \\ 0 \\ \vdots \\ 0 \end{bmatrix}$$
  }

  \item Using the $\vec{r}_{j}$'s derived from the previous part, how can you write out the matrix equation $S_{i} = Q_{i} R$ where $R$ is an upper triangular matrix.

  \ans{
    We can aggregate the vectors $\vec{s}_{j}$ for $j = 1, \cdots i$ into the matrix $S_{i}$ and since all of the $\vec{s}_{j}$ were written as a linear combination of vectors in the $Q_{i}$ matrix, the $R$ matrix can be constructed by aggregating the $\vec{r}_{j}$ vectors.
    $$S_{i} = \begin{bmatrix}
    | & | & | \\
    \vec{s}_{1} & \cdots & \vec{s}_{i} \\
    | & | & |
    \end{bmatrix} = \begin{bmatrix}
    | & | & | \\
    \vec{q}_{1} & \cdots & \vec{q}_{i} \\
    | & | & |
    \end{bmatrix} \cdot \begin{bmatrix}
    \langle\vec{q}_{1}, \vec{s}_{1}\rangle & \cdots & \langle\vec{q}_{1}, \vec{s}_{i}\rangle \\
    \vdots & \ddots & \vdots \\
    0 & 0 & \langle\vec{q}_{i}, \vec{s}_{i}\rangle
    \end{bmatrix}$$
  }


  \item What are the reprsentations of $\hat{y}$ using Q-basis coordinates and standard basis coordinates?

  \ans{
    The estimate $\hat{y}$ using coordinates from the $Q$-basis can be represented as:
    $$ \hat{y} = Q_{i} [\hat{x}]_{Q} = \begin{bmatrix}
    | & | & | \\
    \vec{q}_{1} & \cdots & \vec{q}_{m} \\
    | & | & |
    \end{bmatrix} \begin{bmatrix} x_{q_{1}} \\ \vdots \\ x_{q_{m}} \end{bmatrix} $$
    In standard basis coordinates, it will be: 
    $$ \hat{y} = S \hat{x} = \begin{bmatrix}
    | & | & | \\
    \vec{s}_{1} & \cdots & \vec{s}_{m} \\
    | & | & |
    \end{bmatrix} \begin{bmatrix} x_{1} \\ \vdots \\ x_{m} \end{bmatrix} $$
    Equating the two, we see that:
    $S \hat{x} = Q_{i} [\hat{x}]_{Q}$
  }

  \item How can we use our relation $S_{i} = Q_{i} R$ to solve for the solution $\hat{x}$ in standard basis coordinates?

  \ans{
    Substituting $S_{i} = Q_{i} R,$ we see that
    $$Q_{i} R \hat{x} = Q_{i} [\hat{x}]_{Q}$$
    Since $Q_{i}$ has orthonormal columns, we know that $Q_{i}^{T} Q_{i} = I.$ Therefore, if we left multiply by $Q_{i}^{T}$
    $$Q_{i}^{T} Q_{i} R \hat{x} = R \hat{x} = Q_{i}^{T} Q_{i} [\hat{x}]_{Q} = [\hat{x}]_{Q}$$
    Therefore, taking $R^{-1}$ on both sides, we see that
    $$\hat{x} = R^{-1} [\hat{x}]_{Q}$$
  }

  \item How can we use the fact that $R$ is upper triangular to solve for $\hat{x}$ efficiently?

  \ans{
    Since $R$ is upper triangular, it is already in reduced echlon form. That means we can back-substitute to solve for $\hat{x}$ which is much more efficient than taking the inverse of $R.$ If $R$ is an $n \times n$ matrix, taking the inverse will require $O(n^{3})$ operations while back-substitution will only require $O(n^{2})$ operations. \\
    You can read more about the comparison of matrix inversion vs back-substitution here: \\
    \url{https://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations}
  }

\end{enumerate}









% Orthogonal Matching Pursuit is a way to take a large number of transmissions from satellites and to solve the problem of knowing which satellites are transmitting, and what message they are sending.

% Recall from 16A, that for OMP we can model the observed signal, $\vec{y}$ as a signal of length $N$ and we have a total of $m >> N$ satellites. The model for the output $\vec{y}$ would then be:
% \begin{equation}
%     \vec{y} = a_{1} \vec{s}_{1}^{\ (\tau_{1})} + \cdots + a_m \vec{s}_{m}^{\ (\tau_{m})}
% \end{equation}
% For satellite $i,$ we denote $a_{i}$ as the message it is sending, $\vec{s}_{i}$ as the signal it is transmitting, and $\tau_{i}$ is the delay of the signal from its original form. 

% We then tried setting up a system of equations with $\vec{x}$ as a vector of all of the $a_i,$
% \begin{equation}
% \vec{y} = 
% \begin{bmatrix} 
% | & | & | \\
% \ \vec{s}_{1}^{\ (\tau_{1})} & \cdots & \ \vec{s}_{m}^{\ (\tau_{m})} \\
% | & | & | 
% \end{bmatrix}
% \begin{bmatrix} a_{0} \\ \vdots \\ a_{m} \end{bmatrix} = S \vec{x}. 
% \end{equation}
% We cannot use Least Squares to solve this system since the matrix $S$ is not full rank, since it will have more columns than rows.
% However, if we make the assumption that our solution was sparse ($\vec{x}$ will contain many zeros), then we could use the following iterative algorithm to solve for $\vec{x}.$

% \textbf{Iteration 1:}
% \begin{enumerate}[label=(\arabic*)]
%   \item Cross correlate the original signal $\vec{y}$ with all of the signals and find the signal $\vec{s}_{j}^{\ ({\tau_{j}})}$ with the highest cross correlation at time $\tau_{j}.$
%   \item Shift the original signal $\vec{s}_{j}$ to get the shifted signal $\vec{s}_{j}^{\ (\tau_{j})}.$
%   \item Approximate the signal $\hat{y} = a_{j} \vec{s}_{j}^{\ (\tau_{j})}$ and solve for $a_{j}$ using the 1-D Least Squares solution: $\hat{a_{j}} = \frac{1}{\vec{s}_{j}^{T} \vec{s}_{j}} \vec{s}_{j}^{T} \vec{y}.$
%   \item Calculate the residual of the estimate by computing $\vec{r} = \vec{y} - \hat{y} = \vec{y} - \hat{a_{j}} \vec{s}_{j}^{\ (\tau_{j})}.$
% \end{enumerate}

% \textbf{Iteration i = $\{2, \cdots, n\}$:}
% \begin{enumerate}[label=(\arabic*)]
%   \item Cross correlate the original signal $\vec{y}$ with all of the signals and find the signal $\vec{s}_{k}^{\ ({\tau_{k}})}$ with the highest cross correlation at time $\tau_{k}.$
%   \item Shift the original signal $\vec{s}_{k}$ to get the shifted signal $\vec{s}_{k}^{\ (\tau_{k})}.$
%   \item Approximate the signal $\hat{y} = \sum\limits_{l} a_{l} \vec{s}_{l}^{\ (\tau_{l})}$ using the signals we picked in the previous $i - 1$ iterations. 
%   \item This can be formulated as a least squares problem $\hat{y}_{i} = S_{i} \hat{x}$ where $S_{i}$ is a matrix of all $i$ signals we have picked in each iteration. This can then be solved with the Least Squares solution: $\hat{x} = (S_{i}^{T} S_{i})^{-1} S_{i}^{T} \vec{y}
%   .$
%   \item Calculate the residual of the estimate by computing $\vec{r} = \vec{y} - \hat{y}_{i} = \vec{y} - \sum\limits_{l} a_{l} \vec{s}_{l}^{\ (\tau_{l})}.$
%   \item Continue iterating until the residual $\|\vec{r} \|$ is less than some threshold $\epsilon.$
% \end{enumerate}

% Now that we've established the algorithm for OMP, we will use our knowledge of Gram-Schimdt to see how we can speed up OMP.

% \newpage 

% \begin{enumerate}
%   \item If $S_{i}$ were to have orthonormal columns, what would the least squares solution be?

%   \ans{
%     In general, the least squares solution would be $\hat{x} = (S_{i}^{T} S_{i})^{-1} S_{i}^{T} \vec{y}.$ 
%     However, since $S_{i}$ has orthonormal columns, we know that $S_{i}^{T} S_{i} = I.$ 
%     Therefore, our least squares solution is $\hat{x} = S_{i}^{T} \vec{y}.$
%   }

%   \item We've seen that we can use Gram-Schmidt on the columns of $S_{i}$ to make its columns orthonormal.
%   We will call the matrix with orthonormal columns $Q_{i}.$ What is the orthogonal projection matrix that takes a vector $\vec{y}$ in $\mathbb{R}^{n},$ and projects it onto the $\text{Col}(Q_{i})?$

%   \ans{
%     The projection matrix that projected a vector $\vec{b}$ onto the $\text{Col}(A)$ in Least Squares, was shown to be $A (A^{T} A)^{-1} A^{T}.$ Since our matrix $Q_{i}$ has orthonormal columns, we can say that the projection matrix onto $\text{Col}(Q_{i})$ using Least Squares is $Q_{i} (Q_{i}^{T} Q_{i})^{-1} Q_{i}^{T}.$ We can show this is equal to:
%     $$Q_{i} (Q_{i}^{T} Q_{i})^{-1} Q_{i}^{T} = Q_{i} (I)^{-1} Q_{i}^{T} = Q_{i} Q_{i}^{T}$$
%   }

%   \item Now using the projection matrix from the previous part, let's project the vector $\vec{y}$ onto the $\text{Col}(Q_{i}).$
%   What is the resulting projection?

%   \ans{
%     The resulting projection will be $Q_{i} Q_{i}^{T} \vec{y}.$ This can be simplified as follows:
%     \begin{align*}
%     \text{proj}_{\text{Col}(Q_{i})} \vec{y} &= Q_{i} Q_{i}^{T} \vec{y} = Q_{i} \begin{bmatrix} 
%     - & \vec{q}_{1}^{T} & - \\
%     - & \vdots & - \\
%     - & \vec{q}_{i}^{T} & - 
%     \end{bmatrix} \vec{y} = Q_{i} \begin{bmatrix} \vec{q}_{1}^{T} \vec{y} \\ \vdots \\ \vec{q}_{i}^{T} \vec{y} \end{bmatrix} = \begin{bmatrix}
%     | & | & | \\
%     \vec{q}_{1} & \cdots & \vec{q}_{i} \\
%     | & | & |
%     \end{bmatrix} 
%     \begin{bmatrix} \vec{q}_{1}^{T} \vec{y} \\ \vdots \\ \vec{q}_{i}^{T} \vec{y} \end{bmatrix} \\
%     &= (\vec{q}_{1}^{T} \vec{y}) \vec{q}_{1} + \dotsc + (\vec{q}_{i}^{\ T} \vec{y}) \vec{q}_{i}
%     \end{align*}
%   }

%   \item We will now go back to our algorithm for OMP. The resulting projection from the previous part should be the estimate $\hat{y}_{i}.$ Do you notice anything between iteration $i$ and $i+1?$ 

%   \ans{
%     Our estimate at the $i^{\text{th}}$ iteration is $\hat{y}_{i} = (\vec{q}_{1}^{T} \vec{y}) \vec{q}_{1} + \dotsc + (\vec{q}_{i}^{\ T} \vec{y}) \vec{q}_{i}$ \\
%     On the $(i + 1)^{st}$ iteration, our estimate will be $\hat{y}_{i+1} = (\vec{q}_{1}^{T} \vec{y}) \vec{q}_{1} + \dotsc + (\vec{q}_{i+1}^{\ T} \vec{y}) \vec{q}_{i+1}.$ \\
%     The estimates between iteration $i$ and $i + 1$ only differ by $(\vec{q}_{i+1}^{\ T} \vec{y}) \vec{q}_{i+1}!$ 
%     This means we no longer need to take inverses or use Least Squares to solve for our estimates $\hat{y}_{i}$ since we proved in the previous part that $\text{proj}_{\text{Col}(Q_{i})} \vec{y}$ has a closed formula and can be updated every step by adding $(\vec{q}_{i+1}^{\ T} \vec{y}) \vec{q}_{i+1}.$ 
%   }

%   \item Can you come up with a new algorithm for Gram Schmidt with the new speedup shown in the previous part?

%   \ans{
%     Remember that when we perform Gram Schmidt, all of the previous vectors are already orthonormal to each other, so for every new vector coming in, we can subtract its projections to make it orthonormal to the rest. 

%     For our new algorithm, we will keep track of our orthonormal signals in a set $Q$ initialized to be empty. In addition, we will initalize the residual vector $\vec{r} = \vec{y}.$ For the sake of notation, we will again index our signals from $1$ to $i - 1.$

%     \textbf{At each iteration i = 1, ..., n}
%     \begin{enumerate}[label=(\arabic*)]
%       \item Cross correlate the original signal $\vec{y}$ with all of the signals and find the signal $\vec{s}_{i}^{\ ({\tau_{i}})}$ with the highest cross correlation at time $\tau_{i}.$
%       \begin{itemize}
%         \item Also, shift the original signal $\vec{s}_{i}$ to get the shifted signal $\vec{s}_{i}^{\ (\tau_{i})}.$
%       \end{itemize}
%       \item Make this new signal orthogonal to the rest by applying Gram-Schmidt. 
%       \begin{itemize}
%         %\item We will define the vector $\vec{q}_{i} = \vec{s}_{i}^{\ (\tau_{i})} - \sum\limits_{l = 0}^{i - 1} \langle \vec{q_l}, (\vec{s_i}^{\ (\tau_{i})} \vec{q_l})\rangle$
%         \item Reassign $\vec{q}_{i}$ to $\frac{\vec{q}_{i}}{\norm{\vec{q}_{i}}}$ to make sure it has norm 1.
%         \item Add this vector to the set $Q$ (we do this keep track of the previous vectors)
%       \end{itemize}
%       \item Lastly, we calculate the residual, according to our calculations in the previous part
%       \begin{itemize}
%         \item The estimate $\hat{y}_{i}$ was computed as $\hat{y}_{i} = (\vec{q}_{1}^{T} \vec{y}) \vec{q}_{1} + \cdots + (\vec{q}_{i}^{\ T} \vec{y}) \vec{q}_{i}.$
%         \item The residual was defined to be $\vec{r} = \vec{y} - \hat{y}_{i}.$ 
%         \item Therefore to update the residual, we subtract the residual from the one calculated in the previous step by the difference $(\vec{q}_{i}^{\ T} \vec{y}) \vec{q}_{i}$
%       \end{itemize}
%       \item We continue iterating until $\|\vec{r}\|$ < $\epsilon$
%     \end{enumerate}
%   }
% \end{enumerate}
% Author: Taejin Hwang

\qns{Moore-Penrose Pseudoinverse}

% \meta {
%   If students ask about proving the minimality of the Moore-Penrose Pseudo Inverse, refer back to the last part of the SVD Intuition question.
%   The minimality is connected to the $\text{Col}(A^{T})$ and $\text{Nul}(A)$ and picking "weights" that are solely in $\text{Col}(A^{T})$ and making all of the weights for $\text{Nul}(A)$ zero.
% }

We've seen many different ways to solve the matrix-vector equation:
\begin{equation}
  A \vec{x} = \vec{b}
\end{equation}
We could use Gaussian-Elimination to systematically solve for $\vec{x},$ but we came up with better techniques each depending on the shape of an $m \times n$ matrix $A.$
\begin{itemize}
  \item If $A$ was a square, invertible, matrix, we saw that $\vec{x} = A^{-1} \vec{b}$
  \item If $A$ was full rank, and $m > n,$ we could use Least Squares to say that $\hat{x} = (A^{T} A)^{-1} A^{T} \vec{b}.$
  \item If $A$ had more columns than rows, or $n > m$ and the solution was "sparse," then we could use OMP to solve for $x.$
\end{itemize}
We will now come up with an alternate solution for when $n > m$ called the \textbf{Moore-Penrose} pseudoinverse.

We denote the pseudoinverse with a dagger, and the solution will be $\vec{x} = A^{\dagger} \vec{b}.$
This solution is special in that it has \textbf{minimum norm.} That is, if we have another solution $\vec{z},$ such that $A \vec{z} = \vec{b},$ then $\norm{x} \leq \norm{z}.$

\begin{enumerate}
  \qitem Let's start with the original equation $A \vec{x} = \vec{b}.$ What is the formula for the SVD of $A?$

  \ws{
    \vspace{100px}
  }

  \sol {
    The SVD of $A$ is $A = U \Sigma V^{T}.$
  }

  \qitem We will first assume that $A$ is full row rank or $\text{Rank}(A) = m$ and try undoing the SVD. What will the $\Sigma$ matrix look like?

  \ws{
    \vspace{100px}
  }

  \sol {
    Since $A$ is an $m \times n$ matrix, $\Sigma$ will be also be $m \times n.$
    We assumed that $n > m$, and that $A$ is full rank, so $\Sigma$ will be a matrix with more columns than rows but will have a nonzero entry $\sigma_{i}$ in the $i^{th}$ row and column.
    $\Sigma = \begin{bmatrix} \sigma_{1} & 0 & \cdots & 0 & \cdots & 0 \\ 0 & \sigma_{2} & \cdots & 0 & \cdots & 0 \\ \vdots & \vdots & \ddots & \sigma_{m} & \cdots &  0  \\ \end{bmatrix}$
  }

  \qitem Note that this matrix $\Sigma$ does not have an inverse since it isn't square.
  We can however, construct a matrix $\widetilde{\Sigma}$ such that $\widetilde{\Sigma} \Sigma = I_{n}.$
  What is this matrix $\widetilde{\Sigma}?$

  \ws{
    \vspace{100px}
  }

  \sol {
    The matrix $\Sigma$ is almost diagonal. It has $0$ entries to the right of the $m^{th}$ column.
    Also, since $A$ is full rank, all of the entries on the diagonal are nonzero.
    It follows that the following matrix will make $\Sigma \widetilde{\Sigma} = I_{m}.$
    $$\widetilde{\Sigma} = \begin{bmatrix} \frac{1}{\sigma_{1}} & 0 &  \cdots & 0 \\ 0 & \frac{1}{\sigma_{2}} & \cdots & 0 \\ \vdots & \vdots & \ddots & \frac{1}{\sigma_{m}} \\
    \vdots & \vdots & \ddots & 0 \\ 0 & 0 & 0 & 0 \end{bmatrix}$$
  }

  \newpage

  \qitem What is special about the $U$ and $V$ matrices in the SVD?

  \ws{
    \vspace{100px}
  }

  \sol {
    The $U$ and $V$ matrices have orthonormal columns, meaning $U^{T} U = I_{m}$ and $V^{T} V = I_{n}$ \\
    It follows from this fact that $U^{-1} = U^{T}$ and $V^{-1} = V^{T}.$
  }

  \qitem Use the special facts in the previous part about the $U, V,$ and $\Sigma$ matrices to "undo" the SVD.

  \ws{
    \vspace{100px}
  }

  \sol {
    We first write out the original equation combined with the SVD:
    $$U \Sigma V^{T} \vec{x} = \vec{b}$$
    Then we can left multiply by $U^{T}$
    $$\Sigma V^{T} \vec{x} = U^{T} \vec{b}$$
    Since $\widetilde{\Sigma} \Sigma = I,$ we can use this matrix to undo the effect of $\Sigma.$
    $$V^{T} \vec{x} = \widetilde{\Sigma} U^{T} \vec{b}$$
    Lastly, we left multiply by $V$ to get
    $$\vec{x} = V \widetilde{\Sigma} U^{T} \vec{b}.$$
    This matrix $V \widetilde{\Sigma}U^{T}$ is referred to as the pseudoinverse.
    $$A^{\dagger} = V \widetilde{\Sigma}U^{T}$$
  }

  \qitem We made the assumption that $A$ was a full rank matrix, but this will not always be the case. Therefore, in the case in which $A$ is of rank $k < m,$ we will use the \textbf{compact} SVD. This was derived in question 2 (g), but can be expressed in the following form:
  \begin{equation}
    A = \sum\limits_{i=0}^{k} \sigma_{i} \vec{u}_{i} \vec{v}_{i}^{T} = U_{k} \Sigma_{k} V_{k}^{T}
  \end{equation}
  In this condensed form, $U_{k}$ is a $m \times k$ matrix, $\Sigma_{k}$ is a $k \times k$ matrix, and $V_{k}^{T}$ is a $k \times n$ matrix.

  \textbf{What is the solution $\vec{x}$ in this case?}

  \meta {
    This uses the compact SVD derived in the SVD intuition question.
  }

  \ws{
    \vspace{100px}
  }

  \sol {
    The exact same procedures can be taken as in the previous part. We can do this since $U_{k}^{T} U_{k} = I_{m}$ and $V_{k}^{T}V_{k} = I_{n}.$ In addition, we can notice that $\Sigma_{k}$ is invertible since it is a diagonal matrix with no zero entries on the diagonal. Therefore the solution will be:
    $$\vec{x} = A^{\dagger} \vec{b} = V_{k} \Sigma_{k}^{-1} U_{k}^{T}\vec{b}$$
  }
\end{enumerate}

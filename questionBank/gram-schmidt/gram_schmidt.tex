% Author: Maxwell Chen
\pgfplotsset{width=7cm,compat=1.16}
\usepgfplotslibrary{polar}

\qns{Gram-Schmidt Process}

The Gram-Schmidt Process is a topic formerly covered in EECS 16A that has since moved into EECS 16B. In essence, consider an arbitrary set $S$ of linearly-independent vectors. It would be easier to work instead with a set of vectors that are mutually orthogonal and of unit length. Recall that vectors are "orthogonal" when the angle between them is 90$^\circ$, and vectors of "unit length" have magnitude = 1.

The term "orthonormal" collapses these two terms into one: a set of vectors is orthonormal \textbf{if and only if}:
    \begin{itemize}
        \item Any pair of vectors are orthogonal (for any vectors $u, v \in S$ where $u \neq v$, the angle between them is 90$^\circ$).
        \item Each vector is of unit length (for vector $v \in $ $S$, $|v| = 1$).
    \end{itemize}

Gram-Schmidt Orthonormalization is a process that allows us to take such a set $S$ of arbitrary, linearly-independent vectors, and create a new set $S'$ of vectors that span the exact same space, yet are now orthonormal.

\begin{enumerate}

\qitem Let us go back to EECS 16A. Suppose we have some equation $A\vec{x} = \vec{b}$ that has no solution. Why does it have no solution? Because $A$ has more rows than columns; it is a "tall" matrix where we have more equations than unknowns. How can we solve for $x$? In other words, what is the formula for least-squares?

\sol {
Multiply both sides by $A^{T}$ to get:
$$A^TA\vec{x} = A^T\vec{b}$$
$A^TA$ is a square matrix that we can show to be invertible (?). So, we multiply both sides by the inverse to get
$$\vec{x} = (A^TA)^{-1}A^T\vec{b}$$
This is the formula for least-squares from EECS 16A.
}

\qitem From the result above, we see that we have to invert $A^TA$ every time. This takes a lot of computational power and is rather slow. If $A$ was an orthonormal matrix, how could this help us?

\sol {
Let $A$ be orthonormal. If this were the case, then $A^TA = I$. Recall that $I^{-1} = I$. This means we don't have to do any work for inverting this matrix, and the least-squares formula reduces to $\vec{x} = A^T\vec{b}$.
}

\qitem We would like $A$ to be orthonormal. We cannot, however, arbitrarily replace $A$ with an orthonormal matrix $A'$. What must be true about $A'$ for the least-squares equation to hold?

\sol {
Both $A$ and $A'$ must span the same space. Geometrically, least-squares projects $\vec{b}$ onto the column space (aka span) of $A$, so if we want the projection to hold when using $A'$, then they must have the same span.
}

\qitem Justify that span($\vec{v_1}, \vec{v_2}$) is the same as span($\vec{v_1}, \vec{v_2} - \alpha\vec{v_1}$).

\sol {
The definition of span is all vectors that can be "reached" using a linear combination of a set of vectors. So,
$$span(\vec{v_1}, \vec{v_2}) = \vec{v}_{i}=\beta_{1} \vec{v}_{1}+\beta_{2} \vec{v}_{2} \text { for arbitrary } \vec{v}_{i} \text { and all real } \beta_{1}, \beta_{2}$$
Following a similar train of thought,
$$\begin{aligned} \operatorname{span}\left(\vec{v}_{1}, \vec{v}_{2}-\alpha \vec{v}_{1}\right) &=\beta_{3} \vec{v}_{1}+\beta_{4}\left(\vec{v}_{2}-\alpha \vec{v}_{1}\right) \\ &=\beta_{3} \vec{v}_{1}+\beta_{4} \vec{v}_{2}-\beta_{4} \alpha \vec{v}_{1} \\ &=\left(\beta_{3}-\beta_{4} \alpha\right) \vec{v}_{1}+\beta_{4} \vec{v}_{2} \end{aligned}$$
All $\alpha$ and $\beta$ are arbitrary constants that we can set. If we set $\beta_4 = \beta_2$ and $\beta_3 = \beta_1 + \alpha\beta_2$, then these spans are the same!
}

\qitem Let us iteratively generate the orthonormal set $S'$ from $S$. Start by arbitrarily picking a vector $\vec{v_1}$ from $S$: how can we create a new vector $\vec{w_1}$ for $S'$ so that it spans the same space as the original vector, yet ensures that orthonormality is preserved for our new set?

\sol {
 Since we are only taking into consideration a single vector, we do not need to worry about orthogonality. So,
$$\vec{w_1} = \frac{\vec{v_1}}{|\vec{v_1}|}$$
}

\qitem Now, consider an arbitrary vector $\vec{v_2}$ in $S$. How can we create $\vec{w_2}$ for our orthonormal set? (Hint: see part (d))

\sol {
Now, we do need to consider orthogonality. In order for two vectors $\vec{u}, \vec{v}$ to be orthogonal, their dot product has to be 0. In other words, this means that there is nothing--no trace, no component--of $\vec{u} \text { in } \vec{v}$, and vice versa. We can do this by defining the following:
$$\vec{p_2} = \vec{v_2} - (\vec{v_2}^T\vec{w_1})\vec{w_1}$$
By subtracting the projection of $\vec{v_2}$ onto $\vec{w_1}$, we are removing the $\vec{w_1}$ component of $\vec{v_2}$ from $\vec{v_2}$. So, $\vec{p_2}$ is now orthogonal to $\vec{w_1}$, and mutual orthogonality is preserved for all vectors in our new set $S'$. $\vec{p_2}$ also needs to be normalized, so:
$$\vec{w_2} = \frac{\vec{p_2}}{|\vec{p_2}|}$$
}

\qitem Write out the process for creating an arbitrary vector $\vec{w_i}$ for our orthonormal set $S'$.

\sol {
Define the following vector:
$$\vec{p_i} = \vec{v_i} - \sum_{j=1}^{i-1}(\vec{v_i}^T\vec{w_j})\vec{w_j}$$
By subtracting the component of every other vector already in $S'$, this ensures that our new vector $\vec{w_i}$ is orthogonal. Then, we normalize this:
 $$\vec{w_i} = \frac{\vec{p_i}}{|\vec{p_i}|}$$
Rinse and repeat for all vectors in $S$ to generate a new, orthonormal set $S'$ that we can use in place of $S$.
}
\end{enumerate}